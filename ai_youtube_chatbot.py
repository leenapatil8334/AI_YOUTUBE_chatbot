# -*- coding: utf-8 -*-
"""ai_youtube_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TYZ1HBEDP9OQD0sqcK8Ob9JBI7cMviTf
"""

!pip install -q langchain-community \
               langchain-huggingface \
               faiss-cpu \
               sentence-transformers \
               transformers \
               python-dotenv

from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
import gradio as gr
from langchain.schema.runnable import RunnablePassthrough
import re
import os
from langchain_core.output_parsers import StrOutputParser

def get_video_id(url):
    if "v=" in url:
        return url.split("v=")[1].split("&")[0]
    elif "youtu.be" in url:
        return url.split("/")[-1]
    return None

def get_video_id(url):
    match = re.search(r"(?:v=|\/)([0-9A-Za-z_-]{11})", url)
    return match.group(1) if match else url.strip()

from youtube_transcript_api import YouTubeTranscriptApi
print(dir(YouTubeTranscriptApi))  # should now have get_transcript

from youtube_transcript_api import YouTubeTranscriptApi

yt = YouTubeTranscriptApi()
available_transcripts = yt.list("Ks-_Mh1QhMc")
print(available_transcripts)

transcript_dicts = [
    {"start": e.start, "duration": e.duration, "text": e.text}
    for e in yt.fetch("Ks-_Mh1QhMc", languages=['en'])
]
print(transcript_dicts[:5])

from youtube_transcript_api import YouTubeTranscriptApi

yt = YouTubeTranscriptApi()

def get_transcript_dict(video_id, lang='en'):
    return [
        {"start": e.start, "duration": e.duration, "text": e.text}
        for e in yt.fetch(video_id, languages=[lang])
    ]

# Example:
transcript = get_transcript_dict("Ks-_Mh1QhMc")

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Join transcript entries into one big string
full_text = " ".join(entry["text"] for entry in transcript)

splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
chunks = splitter.create_documents([full_text])

print(chunks[0].page_content)  # Check first chunk



len(chunks)

chunks[0]

"""



## Step 1c & 1d - Indexing (Embedding Generation and Storing in Vector Store)


"""

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

vector_store = FAISS.from_documents(chunks, embeddings)

vector_store.index_to_docstore_id

vector_store.get_by_ids(['c0cc193e-bfc2-4329-9d33-fd2d4d46a131'])

cached_vector_store = {}

def get_vector_store(video_id, chunks, embeddings):
    if video_id not in cached_vector_store:
        cached_vector_store[video_id] = FAISS.from_documents(chunks, embeddings)
    return cached_vector_store[video_id]

vector_store = get_vector_store("Ks-_Mh1QhMc", chunks, embeddings)







"""# Step 2 - Retrieval

"""

retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})

retriever

"""# Step 3 - Augmentation

1.   List item
2.   List item


"""

from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint
import os

os.environ["HUGGINGFACEHUB_API_TOKEN"] = " "

# Use a model that works well via Hugging Face Inference API
llm = HuggingFaceEndpoint(
    repo_id="meta-llama/Meta-Llama-3-8B-Instruct",  # or any from the table
    task="text-generation"
)

# Wrap for chat-style usage
model = ChatHuggingFace(llm=llm)

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an expert assistant that answers questions based on a YouTube transcript.

Here is the transcript chunk:\n\n{context}

Answer the following question as clearly and specifically as possible:
{question}
"""
)

question          = "is the topic of nuclear fusion discussed in this video? if yes then what was discussed"
retrieved_docs    = retriever.invoke(question)

retrieved_docs

context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
context_text

final_prompt = prompt.invoke({"context": context_text, "question": question})

final_prompt

"""# Step 4 - Generation

> Add blockquote


"""

answer = model.invoke(final_prompt)
print(answer.content)

from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser

def format_docs(retrieved_docs):
  context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)
  return context_text

parallel_chain = RunnableParallel({
    'context': retriever | RunnableLambda(format_docs),
    'question': RunnablePassthrough()
})

parallel_chain.invoke('who is Demis')

parser = StrOutputParser()

main_chain = parallel_chain | prompt | model | parser

main_chain.invoke('what is the Conversation Going on in the video')

def answer_from_youtube(url, question):
    video_id = get_video_id(url)
    if not video_id:
        return "Invalid YouTube URL."

    yt = YouTubeTranscriptApi()

    try:
        # Fetch transcript and convert to dict format
        transcript = [
            {"start": e.start, "duration": e.duration, "text": e.text}
            for e in yt.fetch(video_id, languages=['en'])
        ]
    except Exception as e:
        return f"Error fetching transcript: {e}"

    if not transcript:
        return "No transcript available for this video."

    # Join text and split into chunks
    full_text = " ".join(entry["text"] for entry in transcript)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = splitter.create_documents([full_text])

    # Embed and create FAISS vector store
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = FAISS.from_documents(chunks, embeddings)
    retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 4})

    # Retrieve relevant chunks
    retrieved_docs = retriever.invoke(question)
    context_text = "\n\n".join(doc.page_content for doc in retrieved_docs)

    # Create prompt and get answer
    final_prompt = prompt.invoke({"context": context_text, "question": question})
    answer = model.invoke(final_prompt)

    return answer.content

# Build Gradio interface
with gr.Blocks() as demo:
    gr.Markdown("# YouTube Transcript QA Chatbot")
    url_input = gr.Textbox(label="YouTube Video URL", placeholder="Enter YouTube video URL here")
    question_input = gr.Textbox(label="Question", placeholder="Ask something about the video")
    output = gr.Textbox(label="Answer", interactive=False)

    btn = gr.Button("Get Answer")
    btn.click(fn=answer_from_youtube, inputs=[url_input, question_input], outputs=output)

demo.launch()

import time

# Before optimization (e.g., no caching)
start = time.time()

# Run your slow function here (e.g., build vector store without cache)
vector_store = FAISS.from_documents(chunks, embeddings)

end = time.time()
original_latency = end - start
print(f"Original latency: {original_latency:.2f} seconds")

# After optimization (e.g., with caching)
start = time.time()

# Run your optimized function (e.g., get_vector_store which uses cache)
vector_store = get_vector_store("Ks-_Mh1QhMc", chunks, embeddings)

end = time.time()
optimized_latency = end - start
print(f"Optimized latency: {optimized_latency:.2f} seconds")

# Calculate % latency reduction
latency_reduction = ((original_latency - optimized_latency) / original_latency) * 100
print(f"Latency reduced by: {latency_reduction:.2f}%")

